# Visualisierung und Analyse

## Q-Learning

### Policy-Darstellung

FÃ¼r Q-Learning kann die erlernte Policy grafisch visualisiert werden. Dies erfolgt durch eine animierte Darstellung der Agentenentscheidungen innerhalb der Gitterumgebung. ZusÃ¤tzlich stehen Pfeildiagramme und ZustandsÃ¼bersichten zur VerfÃ¼gung.

Pygame-Animation mit Emojis:
- ğŸš¢ Agent/Schiff
- ğŸ§­ Start, ğŸ Ziel, ğŸª¨ Hindernis
- ğŸ“¦ Pickup (Container-Szenario)
- â†‘â†’â†“â† Policy-Pfeile

### Evaluation

Zur Auswertung des Trainings kÃ¶nnen verschiedene Statistiken erzeugt werden:
- Erfolgsraten Ã¼ber alle Episoden
- Belohnungsverteilung
- GrÃ¼nde fÃ¼r das Beenden von Episoden (z.â€¯B. Ziel erreicht, Timeout, Kollision)

### Szenarienvergleich

Es ist mÃ¶glich, die Ergebnisse aus mehreren Szenarien gegenÃ¼berzustellen. Dabei werden Erfolgskennzahlen und Abbrucharten analysiert und visualisiert.

### Q-Tabellen-Inspektion

FÃ¼r weiterfÃ¼hrende Analysen kann die trainierte Q-Tabelle interaktiv untersucht werden. Die Darstellung kann je nach Anwendungsfall angepasst werden, etwa als Matrixansicht oder als komprimierte Ãœbersicht.

## Deep Q-Learning (DQN)

### Trainingsvisualisierung

Beim DQN-Training werden automatisch verschiedene Plots erzeugt, etwa:
- Entwicklung des Trainingsverlusts (Loss)
- Verlauf der durchschnittlichen Belohnung
- Erfolgsrate im Zeitverlauf
- Zusammenfassende Mehrfachdarstellungen

### Evaluation

Analog zum Q-Learning kann auch die DQN-Policy visualisiert und analysiert werden. Die Darstellung erfolgt konsistent mit der Q-Learning-Ansicht zur Vergleichbarkeit.

### Szenarienvergleich

Ergebnisse aus mehreren Szenarien lassen sich aggregieren und vergleichen. Neben Erfolgskennzahlen kÃ¶nnen auch statistische Metriken und Hardware-Auslastungen dokumentiert werden.

## Algorithmusvergleich

Zur GegenÃ¼berstellung von Q-Learning und DQN werden standardisierte Visualisierungen erstellt. Dazu zÃ¤hlen:

- Durchschnittliche Erfolgsraten pro Szenario
- Durchschnittliche Episodenschritte
- Verteilungen der erzielten Belohnungen
- Differenzanalysen zwischen beiden Methoden

Die Ergebnisse werden tabellarisch und grafisch zusammengefÃ¼hrt und ermÃ¶glichen eine differenzierte Bewertung der Algorithmen.

## Analyseexporte

WÃ¤hrend Training und Evaluation entstehen automatisch exportierte Ausgabedateien wie:
- PDF-Diagramme
- CSV-Tabellen
- Screenshots von AgentenverlÃ¤ufen

Diese dienen der weiteren Analyse, der Dokumentation und dem Vergleich von TrainingsverlÃ¤ufen.

## Konfiguration

Das Visualisierungsverhalten kann Ã¼ber Parameter gesteuert werden:
- Aktivierung oder Deaktivierung interaktiver Fenster
- Animationsgeschwindigkeit
- Exportformat (z.â€¯B. nur PDF statt Live-Plot)

Diese Einstellungen werden zentral in der Konfigurationsdatei verwaltet und gelten fÃ¼r beide Algorithmen.

## Systemarchitektur

Visualisierungen greifen auf gemeinsame Module zurÃ¼ck, etwa zur Anzeige, zur Formatierung und zur Verwaltung von Ausgabepfaden. Unterschiede zwischen Q-Learning und DQN werden intern behandelt, ohne dass sich dies auf die Bedienung auswirkt.
