# RL Ship Navigation: Q-Learning vs Deep Q-Learning

Ein Vergleich zwischen Q-Learning und Deep Q-Learning Algorithmen fÃ¼r autonome Schiffsnavigation in Grid-Umgebungen.

## ğŸ“‹ ProjektÃ¼bersicht

Dieses Projekt vergleicht die Leistung von klassischem Q-Learning und Deep Q-Learning (DQN) in verschiedenen Navigationsszenarien:

- **Statische Umgebung**: Feste Start-, Ziel- und Hindernis-Positionen
- **Dynamische Szenarien**: ZufÃ¤llige Start-, Ziel- oder Hindernis-Positionen  
- **Container-Umgebung**: Pickup/Dropoff-Aufgaben

## ğŸ—ï¸ Projektstruktur

```
ship-navigation-ql-dqn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ shared/           # Gemeinsame Komponenten
â”‚   â”‚   â”œâ”€â”€ config.py     # Gemeinsame Konfiguration
â”‚   â”‚   â””â”€â”€ envs/         # Environment-Implementierungen
â”‚   â”‚       â”œâ”€â”€ grid_environment.py
â”‚   â”‚       â””â”€â”€ container_environment.py
â”‚   â”œâ”€â”€ q_learning/       # Q-Learning Implementation
â”‚   â”‚   â”œâ”€â”€ train.py      # Training
â”‚   â”‚   â”œâ”€â”€ evaluate_policy.py
â”‚   â”‚   â”œâ”€â”€ visualize_policy.py
â”‚   â”‚   â”œâ”€â”€ compare_scenarios.py
â”‚   â”‚   â””â”€â”€ utils/        # Q-Learning spezifische Utils
â”‚   â”œâ”€â”€ dqn/             # Deep Q-Learning Implementation (in Entwicklung)
â”‚   â””â”€â”€ experiments/      # Vergleichsexperimente
â”œâ”€â”€ exports/             # Trainings-Ergebnisse und Plots
â”œâ”€â”€ docs/               # Dokumentation
â””â”€â”€ README.md
```

## ğŸš€ Installation & Setup

### Voraussetzungen
- Python 3.10+
- Git

### Installation

1. **Repository klonen:**
```bash
git clone https://github.com/Ul012/FOM-rl-shipnav-ql-dql.git
cd FOM-rl-shipnav-ql-dql
```

2. **Virtual Environment erstellen:**
```bash
python -m venv C:\venvs\ql-dqn-venv  # Windows
# oder 
python -m venv venv  # Linux/Mac

# Aktivieren:
C:\venvs\ql-dqn-venv\Scripts\activate  # Windows
# oder
source venv/bin/activate  # Linux/Mac
```

3. **Dependencies installieren:**
```bash
pip install -r requirements.txt
```

## ğŸ® Verwendung

### Q-Learning Training

```bash
cd src/q_learning

# Einzelnes Szenario trainieren
python train.py

# Alle Szenarien trainieren
python train_all_scenarios.py

# Policy evaluieren
python evaluate_policy.py

# Policy visualisieren
python visualize_policy.py

# Szenarien vergleichen
python compare_scenarios.py
```

### Konfiguration anpassen

Editiere `src/shared/config.py` fÃ¼r:
- Hyperparameter (Lernrate, Epsilon, Gamma)
- Environment-Settings (Grid-GrÃ¶ÃŸe, Rewards)
- Training-Parameter (Episoden, Max-Steps)

## ğŸ“Š Szenarien

### 1. Statisches Grid (`static`)
- Feste Positionen fÃ¼r Start, Ziel und Hindernisse
- Baseline fÃ¼r Vergleiche

### 2. ZufÃ¤lliger Start (`random_start`)
- Start-Position wird zufÃ¤llig gewÃ¤hlt
- Ziel und Hindernisse bleiben fest

### 3. ZufÃ¤lliges Ziel (`random_goal`)
- Ziel-Position wird zufÃ¤llig gewÃ¤hlt
- Start und Hindernisse bleiben fest

### 4. ZufÃ¤llige Hindernisse (`random_obstacles`)
- Hindernis-Positionen werden zufÃ¤llig gewÃ¤hlt
- Start und Ziel bleiben fest

### 5. Container-Umgebung (`container`)
- Pickup/Dropoff-Aufgaben
- Komplexere Reward-Struktur

## ğŸ“ˆ Ergebnisse

Nach dem Training werden folgende Dateien erstellt:

- **Q-Tabellen**: `q_table_{scenario}.npy`
- **Lernkurven**: `exports/learning_curve_{scenario}.png`
- **Erfolgsraten**: `exports/success_curve_{scenario}.png`
- **Vergleichsreports**: `exports/comparison_report.pdf`
- 
## ğŸ§ª Experimente

### Aktuell verfÃ¼gbar:
- âœ… Q-Learning fÃ¼r alle Szenarien
- âœ… Hyperparameter-Tuning
- âœ… Visualisierung und Evaluation

### In Entwicklung:
- ğŸš§ Deep Q-Learning (DQN) Implementation
- ğŸš§ Direkter Algorithmus-Vergleich
- ğŸš§ Performance-Benchmarks
- ğŸš§ Mkdocs zur Dokumentation
