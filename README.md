# ğŸš¢ Reinforcement Learning: Q-Learning vs Deep Q-Learning in Grid Navigation

Dieses Projekt untersucht und vergleicht tabellenbasiertes Q-Learning mit Deep Q-Learning (DQN) zur Navigation autonomer Agenten in simulierten Gitterumgebungen. Ziel ist eine reproduzierbare Evaluation beider Verfahren unter einheitlichen Rahmenbedingungen.

## ğŸ“ Projektstruktur

```
ship-navigation-ql-dqn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ q_learning/          # Q-Learning-Training, Evaluation, Visualisierung
â”‚   â”œâ”€â”€ dqn/                 # DQN-Training, Evaluation, Visualisierung
â”‚   â”œâ”€â”€ comparison/          # Algorithmusvergleich & Visualisierungen
â”‚   â””â”€â”€ shared/              # Konfigurationen, Umgebungen, utils
â”œâ”€â”€ docs/                    # Dokumentation (MkDocs)
â”œâ”€â”€ exports/                 # Ausgabedateien (PDFs, CSVs, Plots)
â””â”€â”€ README.md                # ProjektÃ¼bersicht
```

## âš™ï¸ Setup

1. Repository klonen  
2. Virtuelle Umgebung erstellen und aktivieren  
3. AbhÃ¤ngigkeiten installieren

```bash
git clone https://github.com/DeinUser/ship-navigation-ql-dqn.git
cd ship-navigation-ql-dqn
python -m venv venv
venv\Scripts\activate     # Windows
source venv/bin/activate    # Linux/Mac
pip install -r requirements.txt
```

## ğŸ§  Trainingssteuerung

### Q-Learning

```bash
cd src/q_learning
python train_all_scenarios.py
```

### Deep Q-Learning

```bash
cd src/dqn
python train_all_scenarios.py --episodes 500 --runs 3
```
### Hinweis zur AusfÃ¼hrung

Die Trainings- und Vergleichsskripte sind auf eine AusfÃ¼hrung direkt aus der Entwicklungsumgebung (z. B. Ã¼ber den â€Runâ€œ-Button in PyCharm) optimiert. Dabei wird die Projektstruktur korrekt erkannt und alle Importe funktionieren ohne weitere Anpassungen.

FÃ¼r die AusfÃ¼hrung Ã¼ber das Terminal sind ggf. zusÃ¤tzliche Konfigurationsschritte erforderlich (z. B. `PYTHONPATH` oder Modulaufruf mit `-m`).

Empfohlen wird daher die Nutzung der bereitgestellten Run-Konfigurationen in PyCharm.


## ğŸ“Š Vergleichsvisualisierung

Es stehen drei Varianten fÃ¼r den visuellen Vergleich der Algorithmen zur VerfÃ¼gung:

1. **Overview-Visualisierung** (`1_compare_algorithms_overview.py`)  
   â†’ FÃ¼hrt die Evaluation durch und speichert die CSV-Datei (`algorithm_comparison_overview.csv`)

2. **2x2 V1** (`compare_algorithms_scientific.py`)  
   â†’ Wissenschaftliches Grid-Layout mit Erfolgsraten, Belohnung, Schritten und Scatterplot  
   **Nutzt die CSV aus Overview-Variante als Grundlage.**

3. **2x2 V2** (`compare_algorithms_heatmap.py`)  
   â†’ Fokus auf Darstellung mit Heatmap  
   **Nutzt ebenfalls die CSV aus Overview-Variante als Grundlage.**

â¡ï¸ **Wichtig:** Die Overview-Variante muss vor den anderen beiden ausgefÃ¼hrt werden.



## ğŸŒ Szenarien

Es werden fÃ¼nf Varianten unterschieden:
- `static` â€“ feste Start-/Ziel-/Hindernispositionen
- `random_start` â€“ zufÃ¤lliger Start
- `random_goal` â€“ zufÃ¤lliges Ziel
- `random_obstacles` â€“ zufÃ¤llige Hindernisse
- `container` â€“ Aufgaben mit Pickup & Dropoff

## ğŸ“Š Ergebnisse

Ergebnisse und Visualisierungen (Lernkurven, Erfolgsraten, Vergleichsplots) werden automatisch im jeweiligen `exports/`-Verzeichnis gespeichert. Q-Tabellen und Modellgewichte werden szenariobezogen abgelegt.

## ğŸ“š Dokumentation

Die technische und inhaltliche Dokumentation ist mit MkDocs aufbereitet:

```bash
mkdocs serve
```

â†’ erreichbar unter [http://localhost:8000](http://localhost:8000)

