# RL Ship Navigation: Q-Learning vs Deep Q-Learning

Ein Vergleich zwischen Q-Learning und Deep Q-Learning Algorithmen fÃ¼r autonome Schiffsnavigation in Grid-Umgebungen.

## ğŸ“‹ ProjektÃ¼bersicht

Dieses Projekt vergleicht die Leistung von klassischem Q-Learning und Deep Q-Learning (DQN) in verschiedenen Navigationsszenarien:

- **Statische Umgebung**: Feste Start-, Ziel- und Hindernis-Positionen
- **Dynamische Szenarien**: ZufÃ¤llige Start-, Ziel- oder Hindernis-Positionen  
- **Container-Umgebung**: Pickup/Dropoff-Aufgaben

## ğŸ§  Algorithmen

### Q-Learning
- Tabellenbasierter Ansatz mit expliziter Q-Tabelle
- Epsilon-Greedy Exploration
- Optimal fÃ¼r kleine, diskrete ZustandsrÃ¤ume

### Deep Q-Learning (DQN)
- Neuronale Netzwerke approximieren Q-Funktion
- Experience Replay fÃ¼r stabiles Training
- Target Network fÃ¼r stabilere Updates
- Skaliert auf grÃ¶ÃŸere ZustandsrÃ¤ume

## ğŸ—ï¸ Projektstruktur

```
ship-navigation-ql-dqn/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ shared/           # Gemeinsame Komponenten
â”‚   â”‚   â”œâ”€â”€ config.py     # Gemeinsame Konfiguration
â”‚   â”‚   â””â”€â”€ envs/         # Environment-Implementierungen
â”‚   â”‚       â”œâ”€â”€ grid_environment.py
â”‚   â”‚       â””â”€â”€ container_environment.py
â”‚   â”œâ”€â”€ q_learning/       # Q-Learning Implementation
â”‚   â”‚   â”œâ”€â”€ train.py      # Training
â”‚   â”‚   â”œâ”€â”€ evaluate_policy.py
â”‚   â”‚   â”œâ”€â”€ visualize_policy.py
â”‚   â”‚   â”œâ”€â”€ compare_scenarios.py
â”‚   â”‚   â””â”€â”€ utils/        # Q-Learning spezifische Utils
â”‚   â”œâ”€â”€ dqn/             # Deep Q-Learning Implementation
â”‚   â”‚   â”œâ”€â”€ deep_q_agent.py
â”‚   â”‚   â”œâ”€â”€ train.py
â”‚   â”‚   â””â”€â”€ train_all_scenarios.py
â”‚   â”œâ”€â”€ comparison/       # Algorithmus-Vergleich
â”‚   â”‚   â””â”€â”€ compare_algorithms.py
â”‚   â””â”€â”€ experiments/      # Vergleichsexperimente
â”œâ”€â”€ exports/             # Trainings-Ergebnisse und Plots
â”œâ”€â”€ docs/               # Dokumentation
â””â”€â”€ README.md
```

## ğŸš€ Installation & Setup

### Voraussetzungen
- Python 3.10+
- Git

### Installation

1. **Repository klonen:**
```bash
git clone https://github.com/Ul012/FOM-rl-shipnav-ql-dql.git
cd FOM-rl-shipnav-ql-dql
```

2. **Virtual Environment erstellen:**
```bash
python -m venv C:\venvs\ql-dqn-venv  # Windows
# oder 
python -m venv venv  # Linux/Mac

# Aktivieren:
C:\venvs\ql-dqn-venv\Scripts\activate  # Windows
# oder
source venv/bin/activate  # Linux/Mac
```

3. **Dependencies installieren:**
```bash
pip install -r requirements.txt
```

## ğŸ® Verwendung

### Q-Learning Training

```bash
cd src/q_learning

# Einzelnes Szenario trainieren
python train.py

# Alle Szenarien trainieren
python train_all_scenarios.py

# Policy evaluieren
python evaluate_policy.py

# Policy visualisieren
python visualize_policy.py

# Szenarien vergleichen
python compare_scenarios.py
```

### Deep Q-Learning Training

```bash
cd src/dqn

# Einzelnes Szenario trainieren
python train.py --mode static --episodes 500

# Alle Szenarien trainieren
python train_all_scenarios.py --episodes 500 --runs 3

# Nur Evaluation (lÃ¤dt gespeichertes Modell)
python train.py --mode static --eval-only
```

### Algorithmus-Vergleich

```bash
cd src/comparison

# VollstÃ¤ndiger Vergleich beider Algorithmen
python compare_algorithms.py --runs 5

# Schneller Test-Vergleich
python compare_algorithms.py --ql-episodes 100 --dqn-episodes 100 --runs 2
```

### Konfiguration anpassen

Editiere `src/shared/config.py` fÃ¼r:
- **Q-Learning**: Lernrate, Epsilon, Gamma
- **DQN**: Network-Architektur, Batch-Size, Experience Replay
- **Environment**: Grid-GrÃ¶ÃŸe, Rewards, Max-Steps
- **Training**: Episoden, Seeds fÃ¼r Reproduzierbarkeit

## ğŸ“Š Szenarien

### 1. Statisches Grid (`static`)
- Feste Positionen fÃ¼r Start, Ziel und Hindernisse
- Baseline fÃ¼r Vergleiche

### 2. ZufÃ¤lliger Start (`random_start`)
- Start-Position wird zufÃ¤llig gewÃ¤hlt
- Ziel und Hindernisse bleiben fest

### 3. ZufÃ¤lliges Ziel (`random_goal`)
- Ziel-Position wird zufÃ¤llig gewÃ¤hlt
- Start und Hindernisse bleiben fest

### 4. ZufÃ¤llige Hindernisse (`random_obstacles`)
- Hindernis-Positionen werden zufÃ¤llig gewÃ¤hlt
- Start und Ziel bleiben fest

### 5. Container-Umgebung (`container`)
- Pickup/Dropoff-Aufgaben
- Komplexere Reward-Struktur

## ğŸ“ˆ Ergebnisse

Nach dem Training werden folgende Dateien erstellt:

**Q-Learning:**
- **Q-Tabellen**: `q_table_{scenario}.npy`
- **Lernkurven**: `exports/learning_curve_{scenario}.png`
- **Erfolgsraten**: `exports/success_curve_{scenario}.png`

**Deep Q-Learning:**
- **Modelle**: `dqn_model_{scenario}.pth`
- **Trainingsverlauf**: `exports/dqn_training_{scenario}.pdf`
- **Verlustkurven**: Integriert in Trainingsplots

**Vergleiche:**
- **Algorithmus-Vergleich**: `exports/algorithm_comparison.pdf`
- **Detaillierte CSV**: `exports/algorithm_comparison.csv`
- **Heatmaps**: `exports/algorithm_heatmap_comparison.pdf`

## ğŸ”§ Troubleshooting

### HÃ¤ufige Probleme

**ModuleNotFoundError:**
```bash
pip install -r requirements.txt
```

**CUDA/GPU Probleme (DQN):**
- DQN erkennt automatisch verfÃ¼gbare Hardware
- Bei Problemen: CPU-Modus in `config.py` forcieren

**Memory Errors (DQN):**
- Reduziere `DQN_BATCH_SIZE` oder `DQN_BUFFER_SIZE` in `config.py`

**Import Errors:**
- FÃ¼hre Befehle aus dem Projekt-Root aus: `python -m src.dqn.train`