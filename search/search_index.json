{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udcd8 Projekt\u00fcbersicht: Q-Learning vs Deep Q-Learning","text":"<p>Dieses Projekt untersucht die Anwendung von Reinforcement Learning zur Navigation autonomer Agenten in Gitterumgebungen. Es werden zwei Ans\u00e4tze implementiert und verglichen:</p> <ul> <li>Q-Learning \u2013 klassisches tabellenbasiertes Verfahren</li> <li>Deep Q-Learning (DQN) \u2013 Approximation \u00fcber neuronale Netze</li> </ul> <p>Ziel ist ein reproduzierbarer Vergleich in mehreren Szenarien mit variabler Komplexit\u00e4t.</p>"},{"location":"#dokumentationsstruktur","title":"\ud83d\udd0d Dokumentationsstruktur","text":"Thema Beschreibung \u2699\ufe0f Setup Einrichtung, Abh\u00e4ngigkeiten, Projektstart \ud83e\udde0 Funktionsweise Technische Grundlagen beider Algorithmen \ud83c\udfaf Training Trainingsdurchf\u00fchrung und Konfigurationsm\u00f6glichkeiten \ud83d\udcca Visualisierung Darstellung und Vergleich von Ergebnissen \ud83d\udcda Entwicklung Hinweise zur Projektstruktur und Erweiterung"},{"location":"#unterstutzte-szenarien","title":"\ud83d\uddfa\ufe0f Unterst\u00fctzte Szenarien","text":"<ul> <li>Static: Feste Start-/Zielposition</li> <li>Random Start: Zuf\u00e4lliger Startpunkt</li> <li>Random Goal: Zuf\u00e4lliges Ziel</li> <li>Random Obstacles: Zuf\u00e4llige Hindernisse</li> <li>Container: Aufgabe mit Pickup und Dropoff</li> </ul>"},{"location":"#vergleichsoptionen","title":"\ud83e\uddea Vergleichsoptionen","text":"<p>Das Projekt enth\u00e4lt drei Varianten zur algorithmischen Ergebnisvisualisierung. Die zentrale Auswertung erfolgt \u00fcber <code>1_compare_algorithms_overview.py</code>. Auf Basis der erzeugten CSV k\u00f6nnen weitere Visualisierungs-Varianten erzeugt werden.</p>"},{"location":"dokumentation/","title":"\ud83d\udcda Produkt-Dokumentation und Entwicklungsstruktur","text":""},{"location":"dokumentation/#lokale-dokumentation","title":"Lokale Dokumentation","text":"<pre><code># Start der Dokumentation im Live-Modus\nmkdocs serve\n# Erreichbar unter: http://127.0.0.1:8000\n</code></pre> <pre><code># Generierung statischer HTML-Dateien\nmkdocs build\n</code></pre> <pre><code># Dokumentation mit Auto-Reload auf anderem Port\nmkdocs serve --dev-addr=127.0.0.1:8001\n</code></pre>"},{"location":"dokumentation/#struktur-der-dokumentation","title":"Struktur der Dokumentation","text":"<pre><code>docs/\n\u251c\u2500\u2500 index.md              # Projekt\u00fcbersicht und Algorithmusvergleich\n\u251c\u2500\u2500 setup.md              # Installation und Projektstart\n\u251c\u2500\u2500 funktionsweise.md     # Algorithmische Grundlagen\n\u251c\u2500\u2500 training.md           # Training und Konfiguration\n\u251c\u2500\u2500 visualisierung.md     # Evaluation und Analyse\n\u2514\u2500\u2500 dokumentation.md      # Entwickler- und Wartungsdokumentation\n</code></pre>"},{"location":"dokumentation/#modularer-aufbau","title":"Modularer Aufbau","text":"<p>Das Projekt ist in separaten Modulen strukturiert:</p> <ul> <li><code>q_learning/</code> und <code>dqn/</code>: eigenst\u00e4ndige Algorithmenbereiche</li> <li><code>shared/</code>: Konfiguration, Umgebungen, Utilities</li> <li><code>comparison/</code>: Scripts zur Visualisierung und Evaluation</li> </ul>"},{"location":"dokumentation/#dokumentationssystem","title":"Dokumentationssystem","text":"<p>Die <code>.md</code>-Dateien in <code>docs/</code> sind mit MkDocs kompatibel und dienen der projektnahen Beschreibung f\u00fcr Entwicklung, Analyse und Pr\u00e4sentation.</p>"},{"location":"dokumentation/#erweiterbarkeit","title":"Erweiterbarkeit","text":"<ul> <li>Neue Szenarien lassen sich durch Erg\u00e4nzen in der <code>SCENARIOS</code>-Struktur einf\u00fcgen</li> <li>Vergleichsmetriken und Visualisierungen sind unabh\u00e4ngig von der Trainingsmethode</li> <li>Modelle und Q-Tabellen werden szenariobasiert gespeichert</li> </ul>"},{"location":"dokumentation/#pflege-und-wartung","title":"Pflege und Wartung","text":"<ul> <li>Dokumentation synchron zur Codebasis halten</li> <li>Versionskontrolle f\u00fcr \u00c4nderungen nutzen</li> <li>Einheitliche Begriffe und Formatierung sicherstellen</li> </ul> <p>Die Dokumentation dient als projektnaher Referenzrahmen und sollte fortlaufend mit dem Code synchronisiert werden.</p>"},{"location":"funktionsweise/","title":"Funktionsweise der Algorithmen","text":""},{"location":"funktionsweise/#q-learning","title":"Q-Learning","text":"<ul> <li>Verwendung einer Q-Tabelle zur Entscheidung</li> <li>F\u00fcr jeden Zustand-Aktions-Paar wird ein Q-Wert gespeichert, der iterativ aktualisiert wird.</li> <li>Aktualisierung mittels Bellman-Gleichung</li> <li>Exploration \u00fcber feste oder decaying \u03b5-greedy-Strategie</li> <li>Ziel ist die Ann\u00e4herung an eine optimale Policy, die in jeder Situation die langfristig beste Entscheidung erm\u00f6glicht.</li> </ul> <p>Dabei stehen: - <code>\u03b1</code> f\u00fcr die Lernrate - <code>\u03b3</code> f\u00fcr den Diskontfaktor - <code>r</code> f\u00fcr die beobachtete Belohnung - <code>s'</code> f\u00fcr den Folgezustand</p>"},{"location":"funktionsweise/#entscheidungsstrategie","title":"Entscheidungsstrategie","text":"<p>Zur Auswahl der n\u00e4chsten Aktion wird typischerweise eine Epsilon-Greedy-Strategie verwendet. Diese kombiniert zuf\u00e4llige Exploration mit der Ausnutzung des aktuellen Wissens.</p>"},{"location":"funktionsweise/#zustandsreprasentation","title":"Zustandsrepr\u00e4sentation","text":"<p>Die Zust\u00e4nde werden diskret codiert. Je nach Umgebung bestehen sie aus Positionsinformationen (z. B. Zeile, Spalte) sowie bei erweiterten Umgebungen zus\u00e4tzliche Merkmale wie den Transportstatus eines Objekts.</p>"},{"location":"funktionsweise/#deep-q-learning-dqn","title":"Deep Q-Learning (DQN)","text":"<ul> <li>Verwendung eines neuronalen Netzwerks zur Approximation der Q-Funktion</li> <li>Techniken: Experience Replay, Target Network</li> <li>Training mittels Mini-Batches und MSE-Loss</li> <li>Unterst\u00fctzt kontinuierlichere Zustandsr\u00e4ume als tabellarisches Q-Learning</li> </ul>"},{"location":"funktionsweise/#hauptkomponenten","title":"Hauptkomponenten","text":"<ol> <li>Q-Network: Vorw\u00e4rtspass zur Sch\u00e4tzung der Q-Werte</li> <li>Target Network: Stabilisierte Zielwerte durch verz\u00f6gertes Update</li> <li>Experience Replay: Training auf zuf\u00e4lligen Mini-Batches aus einer Replay-Memory</li> </ol>"},{"location":"funktionsweise/#lernprozess","title":"Lernprozess","text":"<p>Der Trainingsprozess basiert auf der Minimierung des Fehlers zwischen dem gesch\u00e4tzten Q-Wert und einem Zielwert:</p> <p>Zur Trainingszeit wird ein Zielwert berechnet, auf den sich das Q-Netzwerk iterativ anpasst. Die Netzwerkarchitektur kann flexibel angepasst werden und besteht typischerweise aus mehreren vollst\u00e4ndig verbundenen Schichten mit ReLU-Aktivierung.</p>"},{"location":"funktionsweise/#vergleich-der-ansatze","title":"Vergleich der Ans\u00e4tze","text":"Eigenschaft Q-Learning Deep Q-Learning Q-Funktion Tabelle Neuronales Netzwerk Speicherbedarf Zustands-Aktions-Tabelle Modellparameter Aktualisierung Einzelne Updates Batch-Training mit Replay Zustandsraumgr\u00f6\u00dfe Klein, diskret Gro\u00df, kontinuierlich Konvergenzverhalten Theoretisch garantiert Approximativ, stabilisiert durch Target-Netzwerke"},{"location":"funktionsweise/#gemeinsame-systemkomponenten","title":"Gemeinsame Systemkomponenten","text":""},{"location":"funktionsweise/#umgebungen","title":"Umgebungen","text":"<p>Beide Algorithmen arbeiten mit denselben simulierten Umgebungen: - Einfache Gitter-Navigation - Erweiterte Aufgaben mit Container-Objekt</p>"},{"location":"funktionsweise/#belohnungsstruktur","title":"Belohnungsstruktur","text":"<p>Belohnungen werden in Abh\u00e4ngigkeit der Agenteninteraktion mit der Umgebung vergeben. Belohnungsarten sind unter anderem: - Bewegungskosten - Zielerreichung - Hindernis-Kollision - Abbruch bei Wiederholungen - Aufnehmen und Abliefern von Objekten</p> <p>Die genauen Werte sind konfigurierbar und k\u00f6nnen \u00fcber die zentrale Konfigurationsdatei angepasst werden.</p>"},{"location":"funktionsweise/#terminierungskriterien","title":"Terminierungskriterien","text":"<p>Eine Episode endet unter folgenden Bedingungen: - Ziel wurde erreicht - Ein Abbruchkriterium wurde erf\u00fcllt (z. B. zu viele Schleifen) - Ein Hindernis wurde getroffen - Die maximale Schrittanzahl wurde \u00fcberschritten</p>"},{"location":"funktionsweise/#szenarien","title":"Szenarien","text":"<p>Zur Evaluation werden verschiedene Umgebungsmodi verwendet. Diese variieren beispielsweise durch: - Fixe oder zuf\u00e4llige Start- und Zielpositionen - Platzierung von Hindernissen - Erweiterte Aufgaben wie Objekthandling</p> <p>Die Szenarien lassen sich modular anpassen und sind zentral konfigurierbar.</p>"},{"location":"setup/","title":"Installation und Setup","text":""},{"location":"setup/#voraussetzungen","title":"Voraussetzungen","text":"<p>F\u00fcr das Projekt wird Python 3.8 oder h\u00f6her ben\u00f6tigt. Empfohlen wird die Nutzung einer virtuellen Umgebung. Das Projekt wurde mit Python 3.11.5 entwickelt und getestet.  Die Abh\u00e4ngigkeiten sind in der Datei <code>requirements.txt</code> definiert.</p>"},{"location":"setup/#hardware-unterstutzung","title":"Hardware-Unterst\u00fctzung","text":"<ul> <li>CPU: Alle Algorithmen funktionieren auf Standard-CPUs</li> <li>GPU (optional): DQN unterst\u00fctzt CUDA-beschleunigtes Training bei verf\u00fcgbarer NVIDIA-GPU</li> <li>RAM: Mindestens 4GB empfohlen f\u00fcr gr\u00f6\u00dfere Replay-Buffer</li> </ul>"},{"location":"setup/#einrichtung","title":"Einrichtung","text":"<pre><code># Repository klonen\ngit clone https://github.com/DeinUser/ship-navigation-ql-dqn.git\ncd ship-navigation-ql-dqn\n\n# Virtuelle Umgebung erstellen und aktivieren\npython -m venv venv\nvenv\\Scripts\\activate     # Windows\nsource venv/bin/activate    # Linux/Mac\n\n# Abh\u00e4ngigkeiten installieren\npip install -r requirements.txt\n</code></pre>"},{"location":"setup/#schnelltest","title":"Schnelltest","text":"<p>Zur \u00dcberpr\u00fcfung der Installation kann ein einfacher Importtest durchgef\u00fchrt werden:</p> <pre><code>cd src\npython -c \"from shared.envs import GridEnvironment; print('Setup erfolgreich')\"\n</code></pre>"},{"location":"setup/#strukturuberblick","title":"Struktur\u00fcberblick","text":"<p>Die zentralen Komponenten befinden sich im Ordner <code>src/</code>. F\u00fcr beide Algorithmen gibt es getrennte Trainings- und Evaluationsskripte. Die Dokumentation ist in <code>docs/</code> abgelegt und mit MkDocs aufrufbar.</p> <p>Ergebnisse wie Lernkurven, Erfolgsraten und Vergleichsplots werden automatisch im Verzeichnis <code>exports/</code> gespeichert. F\u00fcr jedes Szenario und jeden Algorithmus wird ein separater Export erzeugt.</p>"},{"location":"training/","title":"Training und Konfiguration","text":""},{"location":"training/#trainingssteuerung","title":"Trainingssteuerung","text":"<p>Das Training erfolgt \u00fcber skriptgesteuerte Abl\u00e4ufe in den jeweiligen Unterordnern <code>q_learning/</code> und <code>dqn/</code>. F\u00fcr jedes Verfahren existiert ein zentrales Trainingsskript (<code>train_all_scenarios.py</code>), das mehrere Umgebungsmodi sequentiell durchl\u00e4uft. Dabei wird jeder Durchlauf mehrfach wiederholt.</p> <p>Das DQN-Training erfolgt objektorientiert \u00fcber die Klasse <code>DQNTrainer</code>, w\u00e4hrend Q-Learning \u00fcber Subprozess-Aufrufe gesteuert wird.</p>"},{"location":"training/#konfigurationsprinzip","title":"Konfigurationsprinzip","text":"<p>Die wichtigsten Hyperparameter sind zentral in <code>config.py</code> hinterlegt. Zugriff und Validierung erfolgen \u00fcber <code>config_utils.py</code>. Parameter wie Lernrate, Epsilon-Strategie, Episodenanzahl oder Seed k\u00f6nnen \u00fcber diese Dateien angepasst werden. </p> <p>Zur flexiblen Ausf\u00fchrung lassen sich bestimmte Parameter zus\u00e4tzlich \u00fcber Kommandozeilenargumente oder Umgebungsvariablen \u00fcbergeben.</p>"},{"location":"training/#trainingsmetriken","title":"Trainingsmetriken","text":"<p>W\u00e4hrend und nach dem Training werden verschiedene Kennzahlen erfasst, unter anderem:</p> <ul> <li>Erfolgsrate \u00fcber alle Episoden</li> <li>Durchschnittliche Schrittanzahl bis zum Ziel</li> <li>Durchschnittlicher Reward pro Episode</li> <li>Explorationseinstellungen und Replay-Speichergr\u00f6\u00dfe (bei DQN)</li> </ul> <p>Die Trainingsverl\u00e4ufe werden pro Szenario als PDF-Dateien gespeichert und zus\u00e4tzlich tabellarisch dokumentiert.</p>"},{"location":"training/#ablaufbeispiel","title":"Ablaufbeispiel","text":"<pre><code>cd src/q_learning\npython train_all_scenarios.py\n</code></pre> <pre><code>cd src/dqn\npython train_all_scenarios.py --episodes 500 --runs 3\n</code></pre>"},{"location":"visualisierung/","title":"Visualisierung und Analyse","text":""},{"location":"visualisierung/#automatische-diagrammerzeugung","title":"Automatische Diagrammerzeugung","text":"<p>Im Rahmen des Trainings werden automatisch visuelle Ausgaben erzeugt. Dazu geh\u00f6ren:</p> <ul> <li>Lernkurven: Entwicklung des Rewards \u00fcber die Episoden</li> <li>Erfolgsratenverlauf: Anteil erfolgreicher Episoden \u00fcber Zeit</li> <li>Vergleichsplots: Aggregierte Darstellung \u00fcber mehrere Runs</li> </ul> <p>Diese Diagramme werden pro Szenario gespeichert. Zus\u00e4tzlich wird f\u00fcr jede Methode eine kombinierte Kurve \u00fcber alle Szenarien erstellt (Ordner <code>exports/combined</code>).</p>"},{"location":"visualisierung/#vergleichsvisualisierungen","title":"Vergleichsvisualisierungen","text":"<p>Zum algorithmischen Vergleich stehen drei Visualisierungsformen zur Verf\u00fcgung:</p> <ol> <li>Overview-Vergleich (<code>1_compare_algorithms_overview.py</code>)  </li> <li>F\u00fchrt eine standardisierte Evaluation durch  </li> <li>Exportiert CSV-Datei und Vergleichsplot  </li> <li> <p>Grundlage f\u00fcr weitere Visualisierungen</p> </li> <li> <p>Scientific (<code>compare_algorithms_scientific.py</code>)  </p> </li> <li>Darstellung in vier Panels  </li> <li> <p>Erfolgsrate, Reward, Schritte und Scatterplot zur Effizienz</p> </li> <li> <p>Heatmap (<code>compare_algorithms_heatmap.py</code>)  </p> </li> <li>Fokus auf Vergleich einzelner Szenarien  </li> <li>Enth\u00e4lt Erfolgs-Heatmap  </li> <li>Container-Szenario ist enthalten</li> </ol>"},{"location":"visualisierung/#exportformate","title":"Exportformate","text":"<p>Die Ergebnisse werden als <code>.pdf</code> und <code>.csv</code> gespeichert. Je nach Konfiguration k\u00f6nnen zus\u00e4tzlich agentenbasierte Visualisierungen erstellt werden (z.\u202fB. finale Positionen).</p> <p>Die benutzerdefinierten Exporte befinden sich im jeweiligen <code>exports/</code>-Unterverzeichnis pro Algorithmus.</p>"}]}